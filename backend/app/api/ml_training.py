"""AI í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ ë° í•™ìŠµ API"""
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from fastapi.responses import FileResponse
from sqlalchemy.orm import Session
from pathlib import Path
import pandas as pd
from datetime import datetime
from typing import Optional

from app.core.database import get_db
from app.services.excel_template_service import ExcelTemplateService
from app.services.naver_map_service import NaverMapService
from loguru import logger

router = APIRouter()


@router.get("/training/template/download")
def download_ml_training_template():
    """AI í•™ìŠµ ë°ì´í„° Excel í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ"""
    try:
        template_path = ExcelTemplateService.create_ml_training_template()
        
        if not Path(template_path).exists():
            raise HTTPException(status_code=404, detail="í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info(f"ğŸ“¥ í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ: {template_path}")
        
        return FileResponse(
            path=str(template_path),
            filename="AIí•™ìŠµë°ì´í„°_template.xlsx",
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": "attachment; filename*=UTF-8''AI%ED%95%99%EC%8A%B5%EB%8D%B0%EC%9D%B4%ED%84%B0_template.xlsx"
            }
        )
    except Exception as e:
        logger.error(f"âŒ í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {str(e)}")


@router.post("/training/upload")
async def upload_training_data(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    AI í•™ìŠµ ë°ì´í„° ì—‘ì…€ ì—…ë¡œë“œ
    
    ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ê³ , ë„¤ì´ë²„ ì§€ë„ APIë¡œ ê±°ë¦¬/ì†Œìš”ì‹œê°„ì„ ìë™ ê³„ì‚°
    """
    if not file.filename.endswith(('.xlsx', '.xls')):
        raise HTTPException(status_code=400, detail="ì—‘ì…€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    try:
        # íŒŒì¼ ì½ê¸°
        content = await file.read()
        df = pd.read_excel(content)
        
        logger.info(f"ğŸ“Š ì—…ë¡œë“œëœ ë°ì´í„°: {len(df)}ê±´")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (ìˆ˜ì •ë¨: ë°°ì°¨ë²ˆí˜¸, ì£¼ë¬¸ë¬´ê²Œ, ê³ ê°ë§Œì¡±ë„ ì œê±°, ê±°ë¦¬/ì†Œìš”ì‹œê°„ ìë™ê³„ì‚°)
        required_columns = [
            "ë°°ì°¨ì¼ì", "ì°¨ëŸ‰ì½”ë“œ", "ì£¼ë¬¸ë²ˆí˜¸",
            "ì£¼ë¬¸íŒ”ë ˆíŠ¸", "ì¶œë°œì§€ì£¼ì†Œ", "ë„ì°©ì§€ì£¼ì†Œ",
            "ì‹¤ì œë¹„ìš©(ì›)", "ë°°ì°¨ìƒíƒœ"
        ]
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise HTTPException(
                status_code=400,
                detail=f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_columns)}"
            )
        
        # ë„¤ì´ë²„ ì§€ë„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        naver_map = NaverMapService()
        
        # ë°ì´í„° ê²€ì¦ ë° ê±°ë¦¬/ì†Œìš”ì‹œê°„ ìë™ ê³„ì‚°
        valid_count = 0
        error_rows = []
        
        # ê±°ë¦¬/ì†Œìš”ì‹œê°„ ì»¬ëŸ¼ ì¶”ê°€
        df["ê±°ë¦¬(km)"] = None
        df["ì‹¤ì œì†Œìš”ì‹œê°„(ë¶„)"] = None
        
        for idx, row in df.iterrows():
            try:
                # ê¸°ë³¸ ê²€ì¦
                if pd.isna(row["ì°¨ëŸ‰ì½”ë“œ"]) or pd.isna(row["ì£¼ë¬¸ë²ˆí˜¸"]):
                    error_rows.append(f"í–‰ {idx + 2}: ì°¨ëŸ‰ì½”ë“œ ë˜ëŠ” ì£¼ë¬¸ë²ˆí˜¸ ëˆ„ë½")
                    continue
                
                # ì£¼ì†Œ ê²€ì¦
                if pd.isna(row["ì¶œë°œì§€ì£¼ì†Œ"]) or pd.isna(row["ë„ì°©ì§€ì£¼ì†Œ"]):
                    error_rows.append(f"í–‰ {idx + 2}: ì¶œë°œì§€ ë˜ëŠ” ë„ì°©ì§€ ì£¼ì†Œ ëˆ„ë½")
                    continue
                
                # ë„¤ì´ë²„ ì§€ë„ APIë¡œ ê±°ë¦¬/ì†Œìš”ì‹œê°„ ìë™ ê³„ì‚°
                logger.info(f"ğŸ—ºï¸ [{idx + 2}í–‰] ê±°ë¦¬/ì†Œìš”ì‹œê°„ ê³„ì‚° ì¤‘...")
                result = await naver_map.calculate_from_addresses(
                    str(row["ì¶œë°œì§€ì£¼ì†Œ"]),
                    str(row["ë„ì°©ì§€ì£¼ì†Œ"])
                )
                
                if result["distance_km"] and result["duration_minutes"]:
                    df.at[idx, "ê±°ë¦¬(km)"] = result["distance_km"]
                    df.at[idx, "ì‹¤ì œì†Œìš”ì‹œê°„(ë¶„)"] = result["duration_minutes"]
                    logger.info(f"   âœ… ê±°ë¦¬: {result['distance_km']}km, ì†Œìš”ì‹œê°„: {result['duration_minutes']}ë¶„")
                else:
                    error_rows.append(f"í–‰ {idx + 2}: ê±°ë¦¬/ì†Œìš”ì‹œê°„ ê³„ì‚° ì‹¤íŒ¨ (ì£¼ì†Œ í™•ì¸ í•„ìš”)")
                    continue
                
                valid_count += 1
                
            except Exception as e:
                error_rows.append(f"í–‰ {idx + 2}: {str(e)}")
        
        # í•™ìŠµ ë°ì´í„° ì €ì¥ ê²½ë¡œ
        data_dir = Path(__file__).parent.parent.parent / "data" / "ml_training"
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ íŒŒì¼ëª…
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = data_dir / f"training_data_{timestamp}.xlsx"
        
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì €ì¥
        valid_indices = [i for i in range(len(df)) if df.at[i, "ê±°ë¦¬(km)"] is not None]
        valid_df = df.iloc[valid_indices]
        valid_df.to_excel(output_file, index=False)
        
        logger.info(f"âœ… í•™ìŠµ ë°ì´í„° ì €ì¥: {output_file}")
        logger.info(f"   ìœ íš¨ ë°ì´í„°: {valid_count}ê±´, ì˜¤ë¥˜: {len(error_rows)}ê±´")
        
        return {
            "success": True,
            "total_rows": len(df),
            "valid_rows": valid_count,
            "error_rows": len(error_rows),
            "errors": error_rows[:10],  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
            "saved_file": str(output_file),
            "message": f"{valid_count}ê±´ì˜ í•™ìŠµ ë°ì´í„°ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤ (ê±°ë¦¬/ì†Œìš”ì‹œê°„ ìë™ ê³„ì‚° ì™„ë£Œ)"
        }
        
    except pd.errors.EmptyDataError:
        raise HTTPException(status_code=400, detail="ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/training/start")
async def start_training(
    model_type: str = "dispatch",  # dispatch, demand, failure
    epochs: int = 10,
    batch_size: int = 32,
    db: Session = Depends(get_db)
):
    """
    AI ëª¨ë¸ í•™ìŠµ ì‹œì‘
    
    Args:
        model_type: í•™ìŠµí•  ëª¨ë¸ ì¢…ë¥˜ (dispatch, demand, failure)
        epochs: í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
    """
    
    # í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
    data_dir = Path(__file__).parent.parent.parent / "data" / "ml_training"
    
    if not data_dir.exists() or not list(data_dir.glob("*.xlsx")):
        raise HTTPException(
            status_code=400,
            detail="í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”"
        )
    
    # ìµœì‹  í•™ìŠµ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
    training_files = sorted(data_dir.glob("training_data_*.xlsx"), reverse=True)
    latest_file = training_files[0]
    
    logger.info(f"ğŸ¤– AI í•™ìŠµ ì‹œì‘: {model_type}")
    logger.info(f"   í•™ìŠµ ë°ì´í„°: {latest_file}")
    logger.info(f"   Epochs: {epochs}, Batch Size: {batch_size}")
    
    try:
        # í•™ìŠµ ë°ì´í„° ë¡œë“œ
        df = pd.read_excel(latest_file)
        
        if model_type == "dispatch":
            # ë°°ì°¨ ìµœì í™” ëª¨ë¸ í•™ìŠµ
            result = await train_dispatch_model(df, epochs, batch_size)
        elif model_type == "demand":
            # ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
            result = await train_demand_model(df, epochs, batch_size)
        elif model_type == "failure":
            # ê³ ì¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
            result = await train_failure_model(df, epochs, batch_size)
        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì¢…ë¥˜ì…ë‹ˆë‹¤")
        
        return {
            "success": True,
            "model_type": model_type,
            "training_data_count": len(df),
            "epochs": epochs,
            "batch_size": batch_size,
            **result
        }
        
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


async def train_dispatch_model(df: pd.DataFrame, epochs: int, batch_size: int):
    """ë°°ì°¨ ìµœì í™” ëª¨ë¸ í•™ìŠµ"""
    
    # ê°„ë‹¨í•œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜
    # ì‹¤ì œë¡œëŠ” TensorFlow/PyTorch ëª¨ë¸ í•™ìŠµ ì½”ë“œ êµ¬í˜„
    
    logger.info("ğŸ“Š ë°°ì°¨ ìµœì í™” ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # íŠ¹ì„± ì¶”ì¶œ (ìˆ˜ì •ë¨: ë°°ì°¨ë²ˆí˜¸, ì£¼ë¬¸ë¬´ê²Œ, ê³ ê°ë§Œì¡±ë„ ì œì™¸)
    features = df[[
        "ì£¼ë¬¸íŒ”ë ˆíŠ¸", "ê±°ë¦¬(km)",
        "ì‹¤ì œì†Œìš”ì‹œê°„(ë¶„)", "ì‹¤ì œë¹„ìš©(ì›)"
    ]].values
    
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
    models_dir = Path(__file__).parent.parent.parent / "models"
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "model_type": "dispatch_optimizer",
        "trained_at": datetime.now().isoformat(),
        "data_count": len(df),
        "epochs": epochs,
        "batch_size": batch_size,
        "avg_cost": float(df["ì‹¤ì œë¹„ìš©(ì›)"].mean()),
        "avg_duration": float(df["ì‹¤ì œì†Œìš”ì‹œê°„(ë¶„)"].mean()),
        "avg_distance": float(df["ê±°ë¦¬(km)"].mean()),
    }
    
    import json
    with open(models_dir / "dispatch_optimizer_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    logger.info("âœ… ë°°ì°¨ ìµœì í™” ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
    
    return {
        "message": "ë°°ì°¨ ìµœì í™” ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
        "avg_cost": metadata["avg_cost"],
        "avg_duration": metadata["avg_duration"],
        "avg_distance": metadata["avg_distance"],
    }


async def train_demand_model(df: pd.DataFrame, epochs: int, batch_size: int):
    """ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
    logger.info("ğŸ“Š ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # ì¼ë³„ ì£¼ë¬¸ëŸ‰ ì§‘ê³„
    daily_orders = df.groupby("ë°°ì°¨ì¼ì").size().reset_index(name="ì£¼ë¬¸ìˆ˜")
    
    return {
        "message": "ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
        "total_days": len(daily_orders),
        "avg_daily_orders": float(daily_orders["ì£¼ë¬¸ìˆ˜"].mean()),
    }


async def train_failure_model(df: pd.DataFrame, epochs: int, batch_size: int):
    """ê³ ì¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
    logger.info("ğŸ“Š ê³ ì¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # ì°¨ëŸ‰ë³„ í†µê³„
    vehicle_stats = df.groupby("ì°¨ëŸ‰ì½”ë“œ").agg({
        "ë°°ì°¨ë²ˆí˜¸": "count",
        "ì‹¤ì œì†Œìš”ì‹œê°„(ë¶„)": "mean",
        "ì‹¤ì œë¹„ìš©(ì›)": "mean",
    }).reset_index()
    
    return {
        "message": "ê³ ì¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
        "total_vehicles": len(vehicle_stats),
    }


@router.get("/training/status")
async def get_training_status():
    """í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
    
    models_dir = Path(__file__).parent.parent.parent / "models"
    
    status = {}
    
    # ë°°ì°¨ ìµœì í™” ëª¨ë¸ ìƒíƒœ
    metadata_file = models_dir / "dispatch_optimizer_metadata.json"
    if metadata_file.exists():
        import json
        with open(metadata_file, "r") as f:
            metadata = json.load(f)
            status["dispatch_optimizer"] = {
                "status": "trained",
                "trained_at": metadata.get("trained_at"),
                "data_count": metadata.get("data_count"),
            }
    else:
        status["dispatch_optimizer"] = {
            "status": "not_trained",
            "message": "í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤"
        }
    
    # í•™ìŠµ ë°ì´í„° íŒŒì¼ ëª©ë¡
    data_dir = Path(__file__).parent.parent.parent / "data" / "ml_training"
    if data_dir.exists():
        training_files = sorted(data_dir.glob("training_data_*.xlsx"), reverse=True)
        status["training_data_files"] = [f.name for f in training_files[:5]]  # ìµœê·¼ 5ê°œ
    else:
        status["training_data_files"] = []
    
    return status


@router.get("/training/history")
async def get_training_history():
    """í•™ìŠµ ì´ë ¥ ì¡°íšŒ"""
    
    data_dir = Path(__file__).parent.parent.parent / "data" / "ml_training"
    
    if not data_dir.exists():
        return {
            "total": 0,
            "files": []
        }
    
    training_files = sorted(data_dir.glob("training_data_*.xlsx"), reverse=True)
    
    history = []
    for file in training_files[:10]:  # ìµœê·¼ 10ê°œ
        # íŒŒì¼ëª…ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
        timestamp_str = file.stem.replace("training_data_", "")
        
        try:
            timestamp = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
            
            # íŒŒì¼ í¬ê¸°
            file_size = file.stat().st_size / 1024  # KB
            
            # ë°ì´í„° ê±´ìˆ˜ (ì—‘ì…€ ì½ì–´ì„œ í™•ì¸)
            df = pd.read_excel(file)
            
            history.append({
                "filename": file.name,
                "uploaded_at": timestamp.isoformat(),
                "file_size_kb": round(file_size, 2),
                "data_count": len(df),
            })
        except Exception as e:
            logger.warning(f"íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {file.name} - {e}")
    
    return {
        "total": len(training_files),
        "files": history
    }
